{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset Preparation**"
      ],
      "metadata": {
        "id": "DzCOX_OAsxSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
      ],
      "metadata": {
        "id": "PUcIPAGuC28_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset obtained from Kaggle\n",
        "# https://www.kaggle.com/datasets/ankurzing/sentiment-analysis-for-financial-news\n",
        "\n",
        "# Read data in as a pandas DataFrame\n",
        "path = 'all-data.csv'\n",
        "column_names = ['label', 'text']\n",
        "df = pd.read_csv(path, encoding='latin-1', header=None, names=column_names)\n",
        "data = df\n",
        "labels = data['label'].tolist()\n",
        "sentences = data['text'].tolist()"
      ],
      "metadata": {
        "id": "AiR6XVUOsdDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data\n",
        "\n",
        "# Convert labels to numeric\n",
        "labels = [0 if label == 'negative' else 1 for label in labels] # 0 for neg, 1 for pos\n",
        "\n",
        "train_sentences, test_sentences, train_labels, test_labels = train_test_split(\n",
        "    sentences, labels, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "7HQ-C2QCkAM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and Encode\n",
        "\n",
        "# Initialize BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # pretrained tokenizer\n",
        "\n",
        "train_encodings = tokenizer(train_sentences, truncation=True, padding=True, return_tensors='pt')\n",
        "test_encodings = tokenizer(test_sentences, truncation=True, padding=True, return_tensors='pt')"
      ],
      "metadata": {
        "id": "EVrCvBfxs-jB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to TensorDataset\n",
        "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], torch.tensor(train_labels))\n",
        "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], torch.tensor(test_labels))"
      ],
      "metadata": {
        "id": "6_s-ezdTvmKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Training**"
      ],
      "metadata": {
        "id": "n1qf9lwBxsOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Loaders\n",
        "batch_size = 16\n",
        "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=batch_size)"
      ],
      "metadata": {
        "id": "nXUyXZc9xxhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BERT model for sequence classification\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2) #pos, neg\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oHJ_I6vzSPF",
        "outputId": "9a998d39-9afa-4b14-f01f-e813e207b20f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "epochs = 5\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for batch in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids, attention_mask, labels = [tensor.to(device) for tensor in batch]\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n"
      ],
      "metadata": {
        "id": "iGRdWs250h_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Evaluation\n",
        "\n",
        "model.eval()\n",
        "\n",
        "predictions, true_labels = [], []\n",
        "\n",
        "for batch in test_dataloader:\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    input_ids, attention_mask, labels = batch\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = labels.to('cpu').numpy()\n",
        "\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)"
      ],
      "metadata": {
        "id": "a63USLQk3D8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Results**"
      ],
      "metadata": {
        "id": "lslNcJGxAArP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Metrics\n",
        "predictions = np.concatenate(predictions, axis=0)\n",
        "true_labels = np.concatenate(true_labels, axis=0)\n",
        "preds_flat = np.argmax(predictions, axis=1).flatten()\n",
        "labels_flat = true_labels.flatten()\n",
        "\n",
        "accuracy = accuracy_score(labels_flat, preds_flat)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(labels_flat, preds_flat, average='binary')\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1}')"
      ],
      "metadata": {
        "id": "TBT6R-pu8ta6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a782deb4-90dd-4d59-ea04-7953ff15b09c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9587628865979382\n",
            "Precision: 0.9648526077097506\n",
            "Recall: 0.9895348837209302\n",
            "F1 Score: 0.9770378874856488\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Testing on Examples**"
      ],
      "metadata": {
        "id": "lpfEV40DAItQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import torch\n",
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "metadata": {
        "id": "6bfQq7gMAFOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to scrape and process text from a URL\n",
        "\n",
        "def scrape_text_from_url(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    # Find the relevant text; the exact method might vary depending on website structure\n",
        "    text = soup.find_all('p')  # Assuming the main content is in <p> tags\n",
        "    return ' '.join([para.get_text() for para in text])\n"
      ],
      "metadata": {
        "id": "lcls_gT8AcrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urls = ['https://finance.yahoo.com/news/google-is-deleting-old-gmail-accounts-heres-how-to-save-yours-192619365.html',\n",
        "        'https://finance.yahoo.com/news/1-canada-google-reach-deal-173201247.html',\n",
        "        'https://www.cnbc.com/2023/12/12/us-pension-funds-heavily-invested-in-china-despite-crackdown.html',\n",
        "        'https://www.fnlondon.com/articles/banks-stalling-pay-overhaul-after-bonus-cap-scrap-no-one-wants-to-be-an-outlier-20231211',\n",
        "        'https://www.goldmansachs.com/intelligence/pages/the-global-economy-will-perform-better-than-many-expect-in-2024.html']"
      ],
      "metadata": {
        "id": "LyVCaupuAXqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [scrape_text_from_url(url) for url in urls]\n",
        "\n",
        "\n",
        "# Tokenize and encode the text\n",
        "encodings = tokenizer(texts, truncation=True, padding=True, return_tensors='pt')\n",
        "#print(encodings)\n",
        "dataset = TensorDataset(encodings['input_ids'], encodings['attention_mask'])\n",
        "dataloader = DataLoader(dataset, batch_size=1)  # Batch size of 1 for individual processing"
      ],
      "metadata": {
        "id": "o59wVlfsArj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict sentiment\n",
        "model.eval()\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "8Q6_Zti_BVQw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00347df7-4c3d-4f7c-8b61-f4966e407a56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Results\n",
        "\n",
        "predictions = []\n",
        "for batch in dataloader:\n",
        "    input_ids, attention_mask = [tensor.to(device) for tensor in batch]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    prediction = torch.argmax(logits, dim=1).cpu().numpy()[0]\n",
        "    predictions.append(prediction)\n",
        "\n",
        "# Map numeric predictions back to sentiment labels\n",
        "predicted_labels = ['positive' if prediction == 1 else 'negative' for prediction in predictions]\n",
        "\n",
        "for url, sentiment in zip(urls, predicted_labels):\n",
        "    print(f\"URL: {url}\\nPredicted Sentiment: {sentiment}\\n\")"
      ],
      "metadata": {
        "id": "gJ2QUXutBm36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fe1b072-fc6a-4b26-a31b-667b3ffe73c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URL: https://finance.yahoo.com/news/google-is-deleting-old-gmail-accounts-heres-how-to-save-yours-192619365.html\n",
            "Predicted Sentiment: positive\n",
            "\n",
            "URL: https://finance.yahoo.com/news/1-canada-google-reach-deal-173201247.html\n",
            "Predicted Sentiment: positive\n",
            "\n",
            "URL: https://www.cnbc.com/2023/12/12/us-pension-funds-heavily-invested-in-china-despite-crackdown.html\n",
            "Predicted Sentiment: negative\n",
            "\n",
            "URL: https://www.fnlondon.com/articles/banks-stalling-pay-overhaul-after-bonus-cap-scrap-no-one-wants-to-be-an-outlier-20231211\n",
            "Predicted Sentiment: positive\n",
            "\n",
            "URL: https://www.goldmansachs.com/intelligence/pages/the-global-economy-will-perform-better-than-many-expect-in-2024.html\n",
            "Predicted Sentiment: positive\n",
            "\n"
          ]
        }
      ]
    }
  ]
}